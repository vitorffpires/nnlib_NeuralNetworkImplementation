Neural Network Library using Numpy
This project offers a lightweight and intuitive neural network library built exclusively with Python's numpy and mathematical libraries. It's designed for those who want to understand the underlying mechanics of neural networks without the overhead of larger frameworks.

Overview
The library provides the essential building blocks to create, train, and evaluate neural network models. It's modular, allowing users to mix and match components as needed.

Features:
Activations: A set of activation functions like Sigmoid, ReLU, Tanh, and more.

Layers: Fundamental layers such as Dense (fully connected), Dropout for regularization, and Batch Normalization.

Losses: Functions to calculate the difference between predicted and actual values, including Mean Squared Error and Binary Cross-Entropy.

Metrics: Tools to evaluate the performance of your model, including Accuracy, Precision, Recall, and more.

Models: Define and manage your neural network architecture, primarily through the Sequential model.

Optimizers: Algorithms to adjust network weights iteratively, featuring SGD and Adam.

Regularization: Techniques to prevent overfitting, including L2 regularization.# nnlib_NeuralNetworkImplementation
 Criação de uma biblioteca de machine learning
