{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnlib.activation_functions.relu import ReLu\n",
    "from nnlib.activation_functions.leaky_relu import LeakyReLu\n",
    "from nnlib.activation_functions.linear import Linear\n",
    "from nnlib.activation_functions.sigmoid import Sigmoid\n",
    "from nnlib.activation_functions.tanh import Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [0.  0.  0.  0.5 1. ]\n",
      "Backward result: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test ReLu activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the ReLu (Rectified Linear Unit) activation function. \n",
    "The ReLu activation is a widely used activation function in neural networks and is defined as \n",
    "f(x) = max(0, x), where x is the input.\n",
    "\n",
    "Usage:\n",
    "- Create a ReLu activation function object using the `relu.ReLu()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = relu.ReLu()  # Create a ReLu activation function\n",
    "    result = activation.activate(0.5)  # Activate the input value 0.5\n",
    "    print(result)  # Print the result of the ReLu activation\n",
    "\n",
    "This code snippet serves as a basic test of the ReLu activation function by applying it to a \n",
    "single input value and displaying the output.\n",
    "'''\n",
    "activation = ReLu()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [-0.01  -0.005  0.     0.5    1.   ]\n",
      "Backward result: [0.01 0.01 0.01 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Leaky ReLu activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Leaky ReLu (Rectified Linear Unit) activation function. \n",
    "The Leaky ReLu activation is a variant of ReLu that allows a small, non-zero gradient for negative inputs, \n",
    "preventing neurons from dying during training. It is defined as:\n",
    "f(x) = x if x > 0\n",
    "f(x) = alpha * x if x <= 0\n",
    "where alpha is a small positive constant.\n",
    "\n",
    "Usage:\n",
    "- Create a Leaky ReLu activation function object using the `leaky_relu.LeakyReLu()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = leaky_relu.LeakyReLu()  # Create a Leaky ReLu activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Leaky ReLu activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = LeakyReLu(0.01)\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [-1.  -0.5  0.   0.5  1. ]\n",
      "Backward result: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Linear activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Linear activation function. The Linear activation function\n",
    "simply returns the input value without introducing non-linearity. It is defined as f(x) = x, where x is the input.\n",
    "\n",
    "Usage:\n",
    "- Create a Linear activation function object using the `linear.Linear()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = linear.Linear()  # Create a Linear activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Linear activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = Linear()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [0.26894142 0.37754067 0.5        0.62245933 0.73105858]\n",
      "Backward result: [0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Sigmoid activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Sigmoid activation function. \n",
    "The Sigmoid activation function maps input values to the range (0, 1) and is commonly used in \n",
    "neural networks for introducing non-linearity.\n",
    "\n",
    "Usage:\n",
    "- Create a Sigmoid activation function object using the `sigmoid.Sigmoid()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = sigmoid.Sigmoid()  # Create a Sigmoid activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Sigmoid activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = Sigmoid()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [-0.76159416 -0.46211716  0.          0.46211716  0.76159416]\n",
      "Backward result: [0.41997434 0.78644773 1.         0.78644773 0.41997434]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Tanh activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Hyperbolic Tangent (Tanh) activation function. \n",
    "The Tanh activation function maps input values to the range (-1, 1) and is commonly used in \n",
    "neural networks for introducing non-linearity.\n",
    "\n",
    "Usage:\n",
    "- Create a Tanh activation function object using the `tanh.Tanh()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = tanh.Tanh()  # Create a Tanh activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Tanh activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = Tanh()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He weight init: [[ 0.19404884 -0.63467524 -1.16021418 -0.27944171 -0.22169271 -0.25084514\n",
      "  -0.28619694 -0.04072481 -0.24111504  0.24400213]\n",
      " [-0.1669611  -0.06181091 -0.36847403 -0.91012208  0.00628119 -0.0205425\n",
      "   0.05659028 -0.62973435  0.00822245  0.15158857]\n",
      " [-0.31551077 -0.03676064  0.06333966  0.04428296  0.39302899  1.09908047\n",
      "  -0.40351347 -0.64650554 -0.40061314  0.25709773]\n",
      " [-0.62994755 -0.74952293  0.6294057   0.01580147 -0.10611228  0.20560344\n",
      "  -0.10451996  0.35719462  0.24713864  0.18760078]\n",
      " [-0.17557638 -0.64183017  1.11628567 -0.25155101 -0.22529044  0.36488446\n",
      "  -0.09433414  0.33151566 -0.51779506  0.21297172]\n",
      " [-0.76192588  0.44920362  0.05329023  0.50898924  0.29673664 -0.64839063\n",
      "   0.04114503  0.08197965  0.2244634   0.09596409]\n",
      " [ 0.523155    0.22095704  0.36462014 -0.27931412 -0.03234861  0.14539519\n",
      "  -0.19661193 -0.683037    0.47443786  0.29243606]\n",
      " [-0.69204646 -0.01605339  0.35816262  0.24896564  0.02243716 -0.10301212\n",
      "  -0.2340451  -1.37822161 -1.11835602 -0.21264574]\n",
      " [-0.47426352 -0.1795141   0.26142473  0.21559371 -0.50787952  0.85370562\n",
      "   0.568965   -0.12702159 -0.13189384 -0.07935388]\n",
      " [-0.24416855  0.01393946  0.14530911 -0.2813588  -0.1427773  -0.04503641\n",
      "   0.82180649 -0.33373079  1.01928137  0.09348038]]\n",
      "Xavier weight init: [[ 0.00147073 -0.1035203  -0.08921622  0.30662642 -0.41603961 -0.31230852\n",
      "   0.37126537  0.44392756 -0.02602887 -0.25561083]\n",
      " [ 0.319035    0.15136335 -0.29811925 -0.37404661 -0.46556217 -0.11042666\n",
      "  -0.28574825  0.00291354 -0.06388993 -0.54120568]\n",
      " [-0.02928872  0.06559173 -0.37711413 -0.19377505 -0.07324953 -0.1682276\n",
      "   0.21054419 -0.43561965 -0.47934216 -0.16784204]\n",
      " [ 0.02109709 -0.18334102 -0.25301373 -0.41771219 -0.08192807  0.44709596\n",
      "   0.33826446  0.38424364 -0.05971151 -0.24861503]\n",
      " [-0.04001957  0.43738408  0.30642444 -0.62641283  0.3874171  -0.21429155\n",
      "  -0.50057334 -0.38781797 -0.05028947  0.19410819]\n",
      " [ 0.16083677 -0.0078445   0.20747012  0.42993988 -0.08518021 -0.236773\n",
      "   0.22818493  1.0686296   0.04775124 -0.37493329]\n",
      " [-0.07434069  0.08627875  0.12224734  0.28746435 -0.28741534 -0.59843101\n",
      "  -0.23778823  0.01810054  0.10615047 -0.06893772]\n",
      " [ 0.08417311  0.18884825  0.05294661  0.25962404 -0.45618702  0.13655368\n",
      "  -0.06971413 -0.13735793 -0.13854365 -0.0887198 ]\n",
      " [ 0.56261156  0.14049748 -0.48756269  0.29613941  0.2064832  -0.10286096\n",
      "   0.21424983 -0.14140839 -0.21748775 -0.02073496]\n",
      " [-0.47722072  0.08271581  0.57829161  0.03542171 -0.0638103  -0.23577254\n",
      "   0.36406198 -0.05052073  0.4837206  -0.18322773]]\n",
      "Uniform weight init: [[ 0.03746117 -0.219909    0.65291476  0.46759878  0.22695986  0.49656677\n",
      "   0.08149525 -0.04035576  0.12634588  0.0020069 ]\n",
      " [-0.45189653 -0.50925953  0.46948181  0.1669697   0.51914597 -0.81415893\n",
      "   0.6459428  -0.67852748 -0.55325508  0.43493125]\n",
      " [-0.158792   -0.26302163  0.33424846  0.28577053 -0.52622213  0.2024537\n",
      "  -0.42951312  0.29347793 -0.3281095  -0.2796372 ]\n",
      " [-0.04494437 -0.40861153 -0.58525679  0.95201756 -0.47531013 -0.55004035\n",
      "  -0.28652547  0.09628273 -0.10113808  0.67112001]\n",
      " [ 0.15937435  0.6907941   0.4144393  -0.17514449 -0.17859599  0.3538041\n",
      "   0.98813766  0.6533669  -0.04041189 -0.90250253]\n",
      " [ 0.4011897   0.05405248 -0.89548943  0.33565935  0.99975559  0.40955443\n",
      "  -0.23705527  0.21009017 -0.44762236  0.37139712]\n",
      " [ 0.78478275  0.07824802  0.83202992  0.96792011 -0.21677482  0.37358277\n",
      "   0.91333559  0.57390943 -0.57438177  0.82098788]\n",
      " [ 0.96226188 -0.73716474 -0.70247063 -0.01834108  0.84581534  0.98143943\n",
      "   0.5036863  -0.24987595 -0.43870334  0.60368779]\n",
      " [ 0.91598061 -0.26186558 -0.11714389 -0.35995784  0.61902978 -0.37261403\n",
      "  -0.38829434 -0.91899154 -0.1515633   0.16135579]\n",
      " [ 0.6415843  -0.98716082 -0.13677419  0.29420905 -0.61631414  0.02295311\n",
      "  -0.43188783  0.87642301  0.57579895 -0.13736823]]\n",
      "Normal weight init: [[ 0.12571065  1.13706005  3.02789324  0.03984136 -0.79000305 -0.19380951\n",
      "   0.75734554  0.23513671 -0.2287577   0.88957751]\n",
      " [-1.21013066 -0.83073407  0.19419007 -0.32117178  0.63115938  1.19124227\n",
      "   1.26473126 -1.61236707 -0.86609715 -0.67231776]\n",
      " [ 0.76288203  0.31158531  1.27420861 -0.37402175  0.70911903 -0.7051478\n",
      "   2.06465352  1.15726258 -0.93735343  0.88690636]\n",
      " [ 1.06275795 -0.25357421  0.86088234 -0.03098592  0.16725874 -0.82253169\n",
      "   0.9535954   0.29524668  1.11506328 -0.28578815]\n",
      " [-0.09772387 -0.31184922 -1.93562779  0.67200959  0.0714298   0.06519815\n",
      "  -0.82018382  0.4839469  -0.28244535 -1.47001548]\n",
      " [-1.49902428  1.12578192  0.66639665  0.2758621  -0.84117422  0.39060935\n",
      "  -0.23441678  0.59852021  0.7240262  -0.59757221]\n",
      " [ 0.23141132 -1.8469447   1.14178132 -0.35914033  0.09265599  1.05115299\n",
      "  -0.6770053  -0.40917831  0.5322858  -0.2112779 ]\n",
      " [-0.79584836 -1.93791206  0.93817027 -1.1357267   0.13440943 -0.68113129\n",
      "   0.22896052  0.12106456  0.48999589 -0.33740169]\n",
      " [-0.484866   -0.44792806 -0.42169559 -2.85902485 -1.14422681  0.87460555\n",
      "   0.45844251 -1.16967996 -0.10836558  0.45943922]\n",
      " [-0.37876341 -0.60577495 -0.29253246  1.37560598  0.22905527  0.12468254\n",
      "  -0.77207897  0.62039353 -0.41064716 -0.51670943]]\n"
     ]
    }
   ],
   "source": [
    "from nnlib.initialization_functions.he import He\n",
    "from nnlib.initialization_functions.xavier import Xavier\n",
    "from nnlib.initialization_functions.uniform import Uniform\n",
    "from nnlib.initialization_functions.normal import Normal\n",
    "\n",
    "he_weight_init = He()\n",
    "xavier_weight_init = Xavier()\n",
    "uniform_weight_init = Uniform()\n",
    "normal_weight_init = Normal()\n",
    "\n",
    "print(\"He weight init: {}\".format(he_weight_init.initialize_weights(10, 10)))\n",
    "print(\"Xavier weight init: {}\".format(xavier_weight_init.initialize_weights(10, 10)))\n",
    "print(\"Uniform weight init: {}\".format(uniform_weight_init.initialize_weights(10, 10)))\n",
    "print(\"Normal weight init: {}\".format(normal_weight_init.initialize_weights(10, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanSquaredError loss: 0.0049999999999999975\n",
      "BinaryCrossEntropyLoss loss: 17.269388197455342\n"
     ]
    }
   ],
   "source": [
    "from nnlib.loss_functions.mse import MeanSquaredError\n",
    "from nnlib.loss_functions.bce import BinaryCrossEntropyLoss\n",
    "\n",
    "\n",
    "mse_loss = MeanSquaredError()\n",
    "bce_loss = BinaryCrossEntropyLoss()\n",
    "\n",
    "print(\"MeanSquaredError loss: {}\".format(mse_loss.compute(np.array([0.5, 0.5]), np.array([0.4, 0.5]))))\n",
    "print(\"BinaryCrossEntropyLoss loss: {}\".format(bce_loss.compute(np.array([0, 1]), np.array([1, 1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22361207 -1.25042797 -1.17753344]\n",
      " [ 1.52458327  0.87892644  0.37108078]\n",
      " [ 1.50317899  0.66795433  0.78657819]]\n",
      "Forward pass output:\n",
      "[[0.84913433 0.59129553 0.56181047]\n",
      " [0.84549868 0.68399023 0.73708336]\n",
      " [0.83587773 0.68499618 0.55753052]]\n",
      "mse_loss: 0.06743250694687958\n",
      "\n",
      "Backward pass output (dLda_prev):\n",
      "[[-0.19022914  0.10020624  0.19833035]\n",
      " [-0.01151022  0.02944589  0.08770224]\n",
      " [-0.1471643   0.07949236  0.1860771 ]]\n"
     ]
    }
   ],
   "source": [
    "from nnlib.layers.dense import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a Dense layer\n",
    "input_dim = 3  # Number of input features\n",
    "n_units = 3    # Number of units in the Dense layer\n",
    "\n",
    "dense_layer = Dense(n_units=n_units, \n",
    "                    activation=Sigmoid(),\n",
    "                    input_dim=input_dim)\n",
    "\n",
    "# Initialize weights and biases manually for testing\n",
    "weights = He().initialize_weights(n_units, input_dim)\n",
    "print(weights)\n",
    "biases = np.array([[0.1, 0.2, 0.3],\n",
    "                   [0.3, 0.4, 0.6],\n",
    "                   [0.5, 0.6, 0.2]])\n",
    "\n",
    "dense_layer.set_weights({'weights': weights, 'biases': biases})\n",
    "\n",
    "# Perform a forward pass\n",
    "input_data = np.array([[0.5, 0.6, 0.4],\n",
    "                       [0.2, 0.1, 0.8],\n",
    "                       [0.3, 0.4, 0.3]])\n",
    "output_data = dense_layer.forward(input_data)\n",
    "\n",
    "print(\"Forward pass output:\")\n",
    "print(output_data)\n",
    "print(f'mse_loss: {mse_loss.compute(np.array([1, 0.5, 1]), output_data)}')\n",
    "\n",
    "# Perform a backward pass\n",
    "dLda = mse_loss.derivate(np.array([1, 0.5, 1]), output_data)\n",
    "dLda_prev = dense_layer.backward(dLda)\n",
    "\n",
    "print(\"\\nBackward pass output (dLda_prev):\")\n",
    "print(dLda_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Data:\n",
      "[[0.5 0.6 0.7]]\n",
      "Initial Weights:\n",
      "[[-0.37665507  0.83652095  0.83678745]\n",
      " [-0.04574435 -0.98661285  0.22485739]\n",
      " [-1.57144775 -0.87314421  0.86909627]]\n",
      "Initial Biases: \n",
      "[[0. 0. 0.]]\n",
      "Forward pass output: \n",
      "[[0.21151999 0.31326303 0.76163704]]\n",
      "\n",
      "Backward pass output (dLda_prev):\n",
      "[[0.00610525 0.02862641 0.0241316 ]]\n",
      "\n",
      "Updated weights:\n",
      "[[-0.37673846  0.83662852  0.83660591]\n",
      " [-0.04584442 -0.98648377  0.22463954]\n",
      " [-1.57156449 -0.87299362  0.8688421 ]]\n",
      "\n",
      "Updated biases:\n",
      "[[-0.00016678  0.00021513 -0.00036309]]\n"
     ]
    }
   ],
   "source": [
    "from nnlib.optimization_functions.sgd import StochastciGradientDescent\n",
    "\n",
    "# Initialize a Dense layer\n",
    "input_dim = 3  # Number of input features\n",
    "n_units = 3    # Number of units in the Dense layer\n",
    "\n",
    "dense_layer = Dense(n_units=n_units, \n",
    "                    activation=Sigmoid(),\n",
    "                    input_dim=input_dim)\n",
    "\n",
    "# Initialize weights and biases manually for testing\n",
    "weights = He().initialize_weights(input_dim, n_units)\n",
    "biases = He().initialize_bias(n_units)\n",
    "\n",
    "dense_layer.set_weights({'weights': weights, 'biases': biases})\n",
    "\n",
    "# Perform a forward pass\n",
    "input_data = np.array([[0.5, 0.6, 0.7]])\n",
    "output_data = dense_layer.forward(input_data)\n",
    "\n",
    "print(\"input Data:\")    \n",
    "print(input_data)\n",
    "\n",
    "print(\"Initial Weights:\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Initial Biases: \")\n",
    "print(biases)\n",
    "\n",
    "print(\"Forward pass output: \")\n",
    "print(output_data)\n",
    "\n",
    "# Assuming mse_loss is defined elsewhere\n",
    "# print(f'mse_loss: {mse_loss.compute(np.array([1, 0.5, 1]), output_data)}')\n",
    "\n",
    "# Perform a backward pass\n",
    "# Dummy derivative of the loss with respect to the activation\n",
    "dLda = np.array([[0.1, -0.1, 0.2]])  \n",
    "dLda_prev = dense_layer.backward(dLda)\n",
    "\n",
    "print(\"\\nBackward pass output (dLda_prev):\")\n",
    "print(dLda_prev)\n",
    "\n",
    "# Initialize SGD optimizer and update the Dense layer's parameters\n",
    "sgd_optimizer = StochastciGradientDescent(learning_rate=0.01)\n",
    "sgd_optimizer.update(dense_layer)\n",
    "\n",
    "# Check the updated weights and biases\n",
    "updated_weights, updated_biases = dense_layer.get_weights()['weights'], dense_layer.get_weights()['biases']\n",
    "print(\"\\nUpdated weights:\")\n",
    "print(updated_weights)\n",
    "print(\"\\nUpdated biases:\")\n",
    "print(updated_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib_test.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m initializer \u001b[39m=\u001b[39m He()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Compile and fit the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m model\u001b[39m.\u001b[39;49mcompile(optimizer, loss, initializer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X, y, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, X_val\u001b[39m=\u001b[39mX_val, y_val\u001b[39m=\u001b[39my_val, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib\\models\\sequential.py:25\u001b[0m, in \u001b[0;36mSequentialModel.compile\u001b[1;34m(self, optimizer, loss, initializer)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Initialize weights if initializer is provided\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m initializer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_weights(initializer)\n",
      "File \u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib\\models\\sequential.py:32\u001b[0m, in \u001b[0;36mSequentialModel._initialize_weights\u001b[1;34m(self, initializer)\u001b[0m\n\u001b[0;32m     28\u001b[0m input_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minput_dim  \u001b[39m# Assuming input_dim for the first layer is set\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Initialize weights and biases\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     weights \u001b[39m=\u001b[39m initializer\u001b[39m.\u001b[39;49minitialize_weights(input_dim, layer\u001b[39m.\u001b[39;49mn_units)\n\u001b[0;32m     33\u001b[0m     biases \u001b[39m=\u001b[39m initializer\u001b[39m.\u001b[39minitialize_bias(layer\u001b[39m.\u001b[39mn_units)\n\u001b[0;32m     35\u001b[0m     \u001b[39m# Set the initialized weights and biases to the layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib\\initialization_functions\\he.py:8\u001b[0m, in \u001b[0;36mHe.initialize_weights\u001b[1;34m(self, input_dim, n_units)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitialize_weights\u001b[39m(\u001b[39mself\u001b[39m, input_dim: \u001b[39mint\u001b[39m, n_units: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39marray:\n\u001b[1;32m----> 8\u001b[0m     std \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m2.\u001b[39;49m \u001b[39m/\u001b[39;49m input_dim)\n\u001b[0;32m      9\u001b[0m     weight_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(\u001b[39m0\u001b[39m, std, size\u001b[39m=\u001b[39m(input_dim, n_units))\n\u001b[0;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m weight_matrix\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Dummy classes for testing\n",
    "from nnlib.layers.dense import Dense\n",
    "\n",
    "from nnlib.optimization_functions.sgd import StochastciGradientDescent\n",
    "\n",
    "from nnlib.models.sequential import SequentialModel\n",
    "\n",
    "from nnlib.loss_functions.mse import MeanSquaredError\n",
    "\n",
    "from nnlib.initialization_functions.he import He\n",
    "\n",
    "# Your SequentialModel class here...\n",
    "\n",
    "# Testing\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# Create dummy data\n",
    "X = np.random.randn(100, 3)\n",
    "y = np.random.randn(100, 1)\n",
    "X_val = np.random.randn(20, 3)\n",
    "y_val = np.random.randn(20, 1)\n",
    "\n",
    "# Initialize model and components\n",
    "model = SequentialModel()\n",
    "model.add(Dense(3, 5))\n",
    "model.add(Dense(5, 1))\n",
    "\n",
    "optimizer = StochastciGradientDescent()\n",
    "loss = MeanSquaredError()\n",
    "initializer = He()\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer, loss, initializer)\n",
    "model.fit(X, y, epochs=10, batch_size=20, X_val=X_val, y_val=y_val, verbose=True)\n",
    "\n",
    "# Save the model\n",
    "model.save_model(\"best_model.pkl\")\n",
    "\n",
    "# Load the model\n",
    "loaded_model = SequentialModel()\n",
    "loaded_model.add(Dense(3, 5))\n",
    "loaded_model.add(Dense(5, 1))\n",
    "loaded_model.compile(optimizer, loss, initializer)\n",
    "loaded_model.load_model(\"best_model.pkl\")\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = loaded_model.predict(X_val)\n",
    "\n",
    "# Ensure to check the predictions, loss values, and best parameters during and after training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
