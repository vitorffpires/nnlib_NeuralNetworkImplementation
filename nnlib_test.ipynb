{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnlib.activation_functions.relu import ReLu\n",
    "from nnlib.activation_functions.leaky_relu import LeakyReLu\n",
    "from nnlib.activation_functions.linear import Linear\n",
    "from nnlib.activation_functions.sigmoid import Sigmoid\n",
    "from nnlib.activation_functions.tanh import Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [0.  0.  0.  0.5 1. ]\n",
      "Backward result: [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test ReLu activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the ReLu (Rectified Linear Unit) activation function. \n",
    "The ReLu activation is a widely used activation function in neural networks and is defined as \n",
    "f(x) = max(0, x), where x is the input.\n",
    "\n",
    "Usage:\n",
    "- Create a ReLu activation function object using the `relu.ReLu()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = relu.ReLu()  # Create a ReLu activation function\n",
    "    result = activation.activate(0.5)  # Activate the input value 0.5\n",
    "    print(result)  # Print the result of the ReLu activation\n",
    "\n",
    "This code snippet serves as a basic test of the ReLu activation function by applying it to a \n",
    "single input value and displaying the output.\n",
    "'''\n",
    "activation = ReLu()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [-0.01  -0.005  0.     0.5    1.   ]\n",
      "Backward result: [0.01 0.01 0.01 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Leaky ReLu activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Leaky ReLu (Rectified Linear Unit) activation function. \n",
    "The Leaky ReLu activation is a variant of ReLu that allows a small, non-zero gradient for negative inputs, \n",
    "preventing neurons from dying during training. It is defined as:\n",
    "f(x) = x if x > 0\n",
    "f(x) = alpha * x if x <= 0\n",
    "where alpha is a small positive constant.\n",
    "\n",
    "Usage:\n",
    "- Create a Leaky ReLu activation function object using the `leaky_relu.LeakyReLu()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = leaky_relu.LeakyReLu()  # Create a Leaky ReLu activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Leaky ReLu activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = LeakyReLu(0.01)\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [-1.  -0.5  0.   0.5  1. ]\n",
      "Backward result: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Linear activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Linear activation function. The Linear activation function\n",
    "simply returns the input value without introducing non-linearity. It is defined as f(x) = x, where x is the input.\n",
    "\n",
    "Usage:\n",
    "- Create a Linear activation function object using the `linear.Linear()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = linear.Linear()  # Create a Linear activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Linear activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = Linear()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [0.26894142 0.37754067 0.5        0.62245933 0.73105858]\n",
      "Backward result: [0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Sigmoid activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Sigmoid activation function. \n",
    "The Sigmoid activation function maps input values to the range (0, 1) and is commonly used in \n",
    "neural networks for introducing non-linearity.\n",
    "\n",
    "Usage:\n",
    "- Create a Sigmoid activation function object using the `sigmoid.Sigmoid()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = sigmoid.Sigmoid()  # Create a Sigmoid activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Sigmoid activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = Sigmoid()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: [-1.  -0.5  0.   0.5  1. ]\n",
      "Forward result: [-0.76159416 -0.46211716  0.          0.46211716  0.76159416]\n",
      "Backward result: [0.41997434 0.78644773 1.         0.78644773 0.41997434]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Test Tanh activation function\n",
    "\n",
    "This code snippet demonstrates the usage of the Hyperbolic Tangent (Tanh) activation function. \n",
    "The Tanh activation function maps input values to the range (-1, 1) and is commonly used in \n",
    "neural networks for introducing non-linearity.\n",
    "\n",
    "Usage:\n",
    "- Create a Tanh activation function object using the `tanh.Tanh()` constructor.\n",
    "- Activate a sample input value (e.g., 0.5) using the `activate()` method and print the result.\n",
    "\n",
    "Example:\n",
    "    activation = tanh.Tanh()  # Create a Tanh activation function\n",
    "    input_value = 0.5\n",
    "    forward_result = activation.activate(input_value)  # Activate the input value 0.5\n",
    "    print(\"Input value: {}\".format(input_value))\n",
    "    print(\"Forward result: {}\".format(forward_result))\n",
    "\n",
    "This code snippet serves as a basic test of the Tanh activation function by applying it to a \n",
    "single input value and displaying the output, both in the forward and backward passes.\n",
    "'''\n",
    "activation = Tanh()\n",
    "input_value = np.array([-1.0,-0.5,0,0.5,1.0])\n",
    "forward_result = activation.activate(input_value)\n",
    "backward_result = activation.derivate(input_value)\n",
    "print(\"Input value: {}\".format(input_value))\n",
    "print(\"Forward result: {}\".format(forward_result))\n",
    "print(\"Backward result: {}\".format(backward_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He weight init: [[-0.23833209 -0.04401495  0.00761981  0.62542551 -0.43188039  0.22306338\n",
      "  -1.01732689  0.71327879 -0.04931432 -0.01654384]\n",
      " [-0.19910085 -0.24994597  0.02072512  0.42806518 -0.06218941 -0.51912847\n",
      "  -0.7067272   0.17574918 -0.540315   -0.20121373]\n",
      " [ 0.5727702   0.36076912 -0.22914252  0.04408876 -0.40457042  0.11427702\n",
      "   0.35243471  0.43815808  0.06592693  0.94569932]\n",
      " [-0.28227588  0.06352193 -0.12042599  0.40260983  0.32438738 -0.26929012\n",
      "  -0.05096352  0.51853177  0.35187066  0.74085559]\n",
      " [-0.92047376  0.14320951  0.87127444  0.53270104  0.37529183 -0.07070329\n",
      "  -0.07958926  0.4212114   0.26332307  0.29726388]\n",
      " [-0.10390607  0.69303542  0.44508955 -0.26245523  0.64803126  0.2972754\n",
      "   0.42720356  0.1510913  -0.05681497 -0.20051583]\n",
      " [-0.68381542  0.33545304 -0.99985011  0.6579545   0.54645644  0.2404405\n",
      "  -0.08446474  0.70475943 -0.37331446  0.18533105]\n",
      " [-0.0745402  -1.11220568  0.24577602  0.02478703  0.40976084  0.0365391\n",
      "  -0.16485022 -0.46297492  0.75974156 -0.06245773]\n",
      " [-0.05821685  0.2323169   0.10260495 -0.04628369  0.07971517 -0.15878189\n",
      "  -0.41532978 -0.57715675 -0.53670897  0.29804149]\n",
      " [ 0.25439892 -0.00939696 -0.13296191 -0.05231531 -0.64820714 -0.69501908\n",
      "   0.49585916  0.47209742 -0.47322679  0.13447788]]\n",
      "Xavier weight init: [[ 0.19729862 -0.30199176 -0.10697296 -0.86023659 -0.26937295  0.03312039\n",
      "  -0.31565787  0.43559934  1.00172878 -0.12594339]\n",
      " [ 0.11127351 -0.05888662  0.15720887  0.17635662 -0.06980162  0.09428155\n",
      "   0.02688341  0.03050792 -0.23540395  0.26887435]\n",
      " [-0.02641649 -0.40043382  0.71634104  0.0731506  -0.105235   -0.02478072\n",
      "   0.14628053 -0.00224315 -0.58460083 -0.07380262]\n",
      " [ 0.03979264  0.10650288  0.0420664  -0.49929404 -0.25990591 -0.02709603\n",
      "   0.16976047 -0.17786146 -0.00794814  0.10634962]\n",
      " [ 0.05322489  0.32403653  0.11700539  0.4143832   0.32979742  0.36474116\n",
      "   0.34271791  0.30864751 -0.06069204  0.35476313]\n",
      " [-0.09261005 -0.60363452  0.11857133 -0.14476337 -0.01069686  0.16282782\n",
      "  -0.02835358 -0.39076913  0.34970165 -0.45822709]\n",
      " [-0.34478868 -0.37758735 -0.10136816 -0.35696222  0.27272343 -0.26459319\n",
      "  -0.04339849  0.04660503  0.24854526 -0.23171376]\n",
      " [ 0.42424433 -0.20666726 -0.1647808  -0.62024987  0.02903875  0.28992056\n",
      "  -0.05320855 -0.09478277 -0.16055224  0.3244105 ]\n",
      " [-0.15974214 -0.19538655  0.14845772  0.01730807  0.15785273 -0.42250927\n",
      "  -0.07243244 -0.01204914  0.65899907  0.04024281]\n",
      " [-0.20088185  0.37474812  0.00325404 -0.03638676 -0.23169797  0.35320787\n",
      "  -0.20728103 -0.31583408 -0.0392175   0.07992634]]\n",
      "Uniform weight init: [[-0.13546871 -0.47927125 -0.2320566   0.07646861  0.44008984 -0.37369858\n",
      "  -0.59033045  0.93836974  0.14437466 -0.76847545]\n",
      " [ 0.99027348  0.05238983 -0.73744196  0.25190537  0.12686616 -0.31558673\n",
      "  -0.83102916 -0.35868538  0.80512899 -0.85210004]\n",
      " [ 0.38229521 -0.71716732  0.90241818  0.04727135  0.04920731 -0.78767505\n",
      "  -0.50875453 -0.8521995   0.96103982 -0.19870625]\n",
      " [-0.97543832 -0.6187512   0.51402777  0.65063363 -0.37436921 -0.55726897\n",
      "   0.5661543   0.98897906 -0.50252791  0.31473143]\n",
      " [-0.06413908 -0.91572828 -0.02163526  0.1906587   0.77178494 -0.40674981\n",
      "   0.43705733 -0.60917736  0.24990511 -0.98848538]\n",
      " [ 0.83327564  0.95915184  0.21042457 -0.32048471  0.90673509  0.02944733\n",
      "  -0.2844448  -0.45572957  0.49245874 -0.68138454]\n",
      " [-0.39899516 -0.659101   -0.18648171 -0.65197936 -0.60107098  0.31809524\n",
      "   0.41841062  0.35682832 -0.29936141  0.47630671]\n",
      " [ 0.13365335 -0.97614012 -0.26187701 -0.32935112 -0.42737805  0.33642007\n",
      "   0.18338239 -0.98030756  0.09111959  0.53757099]\n",
      " [-0.45052519  0.93401902  0.27666091 -0.0482138  -0.72527621  0.50542207\n",
      "   0.34848611 -0.06849169 -0.8520753  -0.49871999]\n",
      " [ 0.28253548 -0.80126808 -0.26384895 -0.12405259 -0.20776607 -0.98515362\n",
      "   0.97348958 -0.41718791 -0.81049919 -0.21540697]]\n",
      "Normal weight init: [[ 1.73591178 -0.48248959  0.22471138 -0.22377933  0.92376212 -0.26404051\n",
      "  -1.2330053   0.27681588  0.60600201 -2.23745719]\n",
      " [-0.78847326  1.52215177  0.74723281  1.42388487 -0.01471722 -0.05695376\n",
      "  -1.13420329  0.90369597 -1.33618887 -0.10103815]\n",
      " [-1.65826742  0.12706232 -1.48229381  0.75796621 -0.85759939 -0.2492025\n",
      "  -0.30996631 -0.69984886  1.05120073 -1.18600857]\n",
      " [ 0.18233112 -0.3756153   0.38988917 -1.02013155  0.18467486 -0.1786061\n",
      "   0.984464   -1.24898543  0.22699018 -0.88915731]\n",
      " [-1.37518036  0.11826827 -0.9444489   1.57860054 -0.555476   -0.89787563\n",
      "  -0.10414009 -2.43290484  0.64282018  0.267622  ]\n",
      " [-0.5014933  -0.26720399 -2.94643097  0.87421508 -1.01903977 -0.2763038\n",
      "   0.56421438  0.07899877 -0.38465862  0.74046626]\n",
      " [-2.36168239  0.26839674  0.20740091 -0.22904877  0.89025616  0.08720896\n",
      "   0.24758906  0.11320484 -1.48048818  1.66794113]\n",
      " [-0.63773283 -2.00709951  2.20185873  0.39616211 -0.5960777  -0.69380356\n",
      "  -0.83061966  2.06169837  1.39960947 -0.45132449]\n",
      " [-0.95425558 -1.46892148  0.43645627 -0.49256073 -0.50397603 -1.37969293\n",
      "   1.27307313  1.19210982 -0.12687106 -1.54512675]\n",
      " [ 0.89355586 -1.63844612  0.03158426 -0.30687342 -0.65342294  0.1148519\n",
      "   0.06076402 -0.35496379 -1.64784023 -0.73569241]]\n"
     ]
    }
   ],
   "source": [
    "from nnlib.initialization_functions.he import He\n",
    "from nnlib.initialization_functions.xavier import Xavier\n",
    "from nnlib.initialization_functions.uniform import Uniform\n",
    "from nnlib.initialization_functions.normal import Normal\n",
    "\n",
    "he_weight_init = He()\n",
    "xavier_weight_init = Xavier()\n",
    "uniform_weight_init = Uniform()\n",
    "normal_weight_init = Normal()\n",
    "\n",
    "print(\"He weight init: {}\".format(he_weight_init.initialize_weights(10, 10)))\n",
    "print(\"Xavier weight init: {}\".format(xavier_weight_init.initialize_weights(10, 10)))\n",
    "print(\"Uniform weight init: {}\".format(uniform_weight_init.initialize_weights(10, 10)))\n",
    "print(\"Normal weight init: {}\".format(normal_weight_init.initialize_weights(10, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanSquaredError loss: 0.0049999999999999975\n",
      "BinaryCrossEntropyLoss loss: 17.269388197455342\n"
     ]
    }
   ],
   "source": [
    "from nnlib.loss_functions.mse import MeanSquaredError\n",
    "from nnlib.loss_functions.bce import BinaryCrossEntropyLoss\n",
    "\n",
    "\n",
    "mse_loss = MeanSquaredError()\n",
    "bce_loss = BinaryCrossEntropyLoss()\n",
    "\n",
    "print(\"MeanSquaredError loss: {}\".format(mse_loss.compute(np.array([0.5, 0.5]), np.array([0.4, 0.5]))))\n",
    "print(\"BinaryCrossEntropyLoss loss: {}\".format(bce_loss.compute(np.array([0, 1]), np.array([1, 1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.12600991  0.77886974  0.68657136]\n",
      " [ 0.14302143  0.48831309 -0.89481576]\n",
      " [ 0.76071747  0.19475219 -1.14572713]]\n",
      "Forward pass output:\n",
      "[[0.74137066 0.72318659 0.41292305]\n",
      " [0.7591597  0.68144209 0.43321424]\n",
      " [0.75458676 0.74789404 0.42661558]]\n",
      "mse_loss: 0.14710899054960602\n",
      "\n",
      "Backward pass output (dLda_prev):\n",
      "[[-0.2375009   0.28414653  0.26807004]\n",
      " [-0.22890922  0.27493102  0.26724411]\n",
      " [-0.22213332  0.2836584   0.27045623]]\n"
     ]
    }
   ],
   "source": [
    "from nnlib.layers.dense import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a Dense layer\n",
    "input_dim = 3  # Number of input features\n",
    "n_units = 3    # Number of units in the Dense layer\n",
    "\n",
    "dense_layer = Dense(n_units=n_units, \n",
    "                    activation=Sigmoid(),\n",
    "                    input_dim=input_dim)\n",
    "\n",
    "# Initialize weights and biases manually for testing\n",
    "weights = He().initialize_weights(n_units, input_dim)\n",
    "print(weights)\n",
    "biases = np.array([[0.1, 0.2, 0.3],\n",
    "                   [0.3, 0.4, 0.6],\n",
    "                   [0.5, 0.6, 0.2]])\n",
    "\n",
    "dense_layer.set_weights({'weights': weights, 'biases': biases})\n",
    "\n",
    "# Perform a forward pass\n",
    "input_data = np.array([[0.5, 0.6, 0.4],\n",
    "                       [0.2, 0.1, 0.8],\n",
    "                       [0.3, 0.4, 0.3]])\n",
    "output_data = dense_layer.forward(input_data)\n",
    "\n",
    "print(\"Forward pass output:\")\n",
    "print(output_data)\n",
    "print(f'mse_loss: {mse_loss.compute(np.array([1, 0.5, 1]), output_data)}')\n",
    "\n",
    "# Perform a backward pass\n",
    "dLda = mse_loss.derivate(np.array([1, 0.5, 1]), output_data)\n",
    "dLda_prev = dense_layer.backward(dLda)\n",
    "\n",
    "print(\"\\nBackward pass output (dLda_prev):\")\n",
    "print(dLda_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Data:\n",
      "[[0.5 0.6 0.7]]\n",
      "Initial Weights:\n",
      "[[ 0.7447731  -0.30910493 -0.49720424]\n",
      " [-0.4062474  -1.23738879 -1.03115752]\n",
      " [-1.49354541  1.07231566 -0.40553674]]\n",
      "Initial Biases: \n",
      "[[0. 0. 0.]]\n",
      "Forward pass output: \n",
      "[[0.28560145 0.46347399 0.24027501]]\n",
      "\n",
      "Backward pass output (dLda_prev):\n",
      "[[ 0.00473001 -0.01516527 -0.0719437 ]]\n",
      "\n",
      "Updated weights:\n",
      "[[ 0.74467108 -0.3089806  -0.49738679]\n",
      " [-0.40636982 -1.23723959 -1.03137657]\n",
      " [-1.49368824  1.07248972 -0.4057923 ]]\n",
      "\n",
      "Updated biases:\n",
      "[[-0.00020403  0.00024867 -0.00036509]]\n"
     ]
    }
   ],
   "source": [
    "from nnlib.optimization_functions.sgd import StochastciGradientDescent\n",
    "\n",
    "# Initialize a Dense layer\n",
    "input_dim = 3  # Number of input features\n",
    "n_units = 3    # Number of units in the Dense layer\n",
    "\n",
    "dense_layer = Dense(n_units=n_units, \n",
    "                    activation=Sigmoid(),\n",
    "                    input_dim=input_dim)\n",
    "\n",
    "# Initialize weights and biases manually for testing\n",
    "weights = He().initialize_weights(input_dim, n_units)\n",
    "biases = He().initialize_bias(n_units)\n",
    "\n",
    "dense_layer.set_weights({'weights': weights, 'biases': biases})\n",
    "\n",
    "# Perform a forward pass\n",
    "input_data = np.array([[0.5, 0.6, 0.7]])\n",
    "output_data = dense_layer.forward(input_data)\n",
    "\n",
    "print(\"input Data:\")    \n",
    "print(input_data)\n",
    "\n",
    "print(\"Initial Weights:\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Initial Biases: \")\n",
    "print(biases)\n",
    "\n",
    "print(\"Forward pass output: \")\n",
    "print(output_data)\n",
    "\n",
    "# Assuming mse_loss is defined elsewhere\n",
    "# print(f'mse_loss: {mse_loss.compute(np.array([1, 0.5, 1]), output_data)}')\n",
    "\n",
    "# Perform a backward pass\n",
    "# Dummy derivative of the loss with respect to the activation\n",
    "dLda = np.array([[0.1, -0.1, 0.2]])  \n",
    "dLda_prev = dense_layer.backward(dLda)\n",
    "\n",
    "print(\"\\nBackward pass output (dLda_prev):\")\n",
    "print(dLda_prev)\n",
    "\n",
    "# Initialize SGD optimizer and update the Dense layer's parameters\n",
    "sgd_optimizer = StochastciGradientDescent(learning_rate=0.01)\n",
    "sgd_optimizer.update(dense_layer)\n",
    "\n",
    "# Check the updated weights and biases\n",
    "updated_weights, updated_biases = dense_layer.get_weights()['weights'], dense_layer.get_weights()['biases']\n",
    "print(\"\\nUpdated weights:\")\n",
    "print(updated_weights)\n",
    "print(\"\\nUpdated biases:\")\n",
    "print(updated_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on layer 1 the output is: [[0.35901725 0.84521284 0.43808681 0.60137579 0.96979654]\n",
      " [0.13989344 0.98868656 0.15482258 0.13218146 0.44104607]\n",
      " [0.73368552 0.6549913  0.2960759  0.39343212 0.60220731]\n",
      " [0.21811769 0.52686905 0.74400263 0.80117505 0.97115762]\n",
      " [0.46064401 0.65997853 0.47228287 0.55038212 0.822745  ]\n",
      " [0.05432435 0.89891334 0.56042197 0.4040085  0.51308736]\n",
      " [0.98516811 0.48432    0.05158391 0.05828666 0.00385863]\n",
      " [0.30183746 0.86171003 0.31835852 0.26013325 0.26544433]\n",
      " [0.98948761 0.52439962 0.09113207 0.33545912 0.80806557]\n",
      " [0.00922044 0.78864434 0.89577906 0.82705862 0.9670374 ]\n",
      " [0.50604864 0.70081278 0.32262177 0.26723503 0.14464842]\n",
      " [0.34871193 0.1053176  0.81047321 0.70369196 0.24721301]\n",
      " [0.1814569  0.93279661 0.32787477 0.29284444 0.5573217 ]\n",
      " [0.96364264 0.24159078 0.17873191 0.20405908 0.02853126]\n",
      " [0.00756986 0.75433652 0.84412222 0.49455064 0.12212165]\n",
      " [0.85652198 0.10542448 0.61247602 0.73546537 0.76450079]\n",
      " [0.54929038 0.20366507 0.57370109 0.4056536  0.04123679]\n",
      " [0.91246497 0.50784407 0.17101766 0.20322605 0.08176673]\n",
      " [0.26846173 0.61929681 0.56810325 0.51606365 0.55075073]\n",
      " [0.86656911 0.39510513 0.30221069 0.38566633 0.33187163]]\n",
      "on layer 2 the output is: [[ 0.47510094  0.20863991  0.17099646 -1.25746081  0.97499964]\n",
      " [ 0.2997513   0.0145081   0.66748718 -0.43726643  0.33420804]\n",
      " [ 0.30513005 -0.09342188 -0.01037541 -1.17179908  0.56312826]\n",
      " [ 0.40544727  0.13251596 -0.47468584 -1.59624391  1.22178318]\n",
      " [ 0.36771502  0.04428935 -0.07890192 -1.28390658  0.89806011]\n",
      " [ 0.29483495 -0.1076134   0.04961635 -0.99684541  0.65792099]\n",
      " [ 0.08564257 -0.42217732 -0.08228388 -0.81640586 -0.13255809]\n",
      " [ 0.2542449  -0.16317201  0.14042506 -0.83556771  0.24178678]\n",
      " [ 0.32696779  0.02899598  0.20437767 -1.05512436  0.6616315 ]\n",
      " [ 0.43698013  0.07834295 -0.40026598 -1.64667621  1.28412493]\n",
      " [ 0.19477181 -0.28106791 -0.12782915 -0.9769585   0.13449609]\n",
      " [ 0.13716677 -0.29448664 -1.27305895 -1.67624373  0.60549086]\n",
      " [ 0.32455962 -0.00389481  0.38532523 -0.76393553  0.54796622]\n",
      " [ 0.06643235 -0.40469821 -0.47613184 -1.03847363 -0.00716462]\n",
      " [ 0.16552035 -0.42252343 -0.65214502 -1.33992362  0.45680151]\n",
      " [ 0.26175115 -0.11076637 -0.92573214 -1.8021166   0.97436256]\n",
      " [ 0.04698165 -0.45981508 -0.92073365 -1.30933682  0.27651596]\n",
      " [ 0.14155867 -0.36511708 -0.23123463 -1.02860081 -0.01048082]\n",
      " [ 0.28195008 -0.08935609 -0.29097796 -1.23177434  0.70987722]\n",
      " [ 0.19929574 -0.2414918  -0.41251288 -1.2623721   0.32403242]]\n",
      "on loss: 1.0559883355701152\n",
      "on loss the derivate is: [[ 3.56325558  3.03033351  2.95504662  0.09813208  4.56305299]\n",
      " [-2.71675877 -3.28724516 -1.981287   -4.19079422 -2.64784528]\n",
      " [ 0.84658819  0.04948434  0.21557726 -2.10727008  1.3625846 ]\n",
      " [ 2.17125096  1.62538832  0.41098473 -1.83213141  3.80392277]\n",
      " [-0.59733612 -1.24418747 -1.49057    -3.90057932  0.46335406]\n",
      " [ 1.51110948  0.70621278  1.02067228 -1.07225124  2.23728155]\n",
      " [ 2.83980209  1.8241623   2.50394919  1.03570522  2.40340076]\n",
      " [ 3.2019248   2.36709099  2.97428513  1.02229958  3.17700858]\n",
      " [-0.73361073 -1.32955434 -0.97879097 -3.49779502 -0.06428331]\n",
      " [ 1.19310713  0.47583278 -0.48138509 -2.97420554  2.88739674]\n",
      " [ 0.65694674 -0.2947327   0.01174483 -1.68651388  0.53639531]\n",
      " [-1.88115407 -2.74446088 -4.70160551 -5.50797506 -0.9445059 ]\n",
      " [ 2.90277086  2.245862    3.02430209  0.72578056  3.34958405]\n",
      " [ 1.5942202   0.65195908  0.50909183 -0.61559176  1.44702627]\n",
      " [ 1.10080031 -0.07528725 -0.53453043 -1.91008762  1.68336264]\n",
      " [ 0.33479913 -0.41023592 -2.04016746 -3.79293637  1.76002194]\n",
      " [ 0.17830621 -0.83528725 -1.75712439 -2.53433073  0.63737482]\n",
      " [ 0.85689172 -0.15645977  0.11130512 -1.48342724  0.55281274]\n",
      " [ 0.68715296 -0.05545937 -0.45870312 -2.34029587  1.54300724]\n",
      " [ 0.61320204 -0.26837305 -0.6104152  -2.31013365  0.8626754 ]]\n",
      "on layer 1 the derivate on the backward is: [[-2.67768447  1.8286466  -3.6863872   0.13587429  8.35744401]\n",
      " [ 4.96923914 -0.75759373  7.30350922  3.59568138 -6.32391236]\n",
      " [ 1.15940264  0.36606197  2.11698686  2.3956168   1.37554157]\n",
      " [ 0.08018107  0.21084274  1.61328702  2.98970742  4.88961031]\n",
      " [ 3.51955287 -0.67521785  6.27257039  4.49878733 -1.6373931 ]\n",
      " [-0.11652201  0.72540901  0.34436561  1.01080968  3.34032701]\n",
      " [-2.49210381  1.7025551  -4.20449817 -1.20834767  5.38587334]\n",
      " [-2.91759546  1.93656718 -4.71784326 -1.2337215   6.75934635]\n",
      " [ 3.16167557 -0.2952112   5.16868632  3.63495658 -1.84938897]\n",
      " [ 1.67540737 -0.24921264  4.14202757  3.99266092  2.61131772]\n",
      " [ 1.14770046  0.30550185  1.75042272  1.9816771   0.27584507]\n",
      " [ 6.35304449 -2.63251295 11.51748864  7.76177183 -6.22228501]\n",
      " [-2.68696225  1.90746104 -4.1691035  -1.25352634  6.85789005]\n",
      " [-0.18654515  0.49798759 -0.11392932  1.20305347  2.20450036]\n",
      " [ 1.30068872 -0.18951752  3.01311046  2.68365093  1.25628942]\n",
      " [ 3.17421687 -1.14961467  6.60073524  5.53952758 -0.18649516]\n",
      " [ 2.51142075 -0.92177109  4.95384178  3.7444181  -1.16140526]\n",
      " [ 0.91681177  0.38624929  1.29748122  1.87153491  0.46228629]\n",
      " [ 1.57101628 -0.14292707  3.27482683  2.99702228  1.03476137]\n",
      " [ 1.72657372 -0.11637947  3.08981833  3.14428187  0.11441932]]\n",
      "on layer 2 the derivate on the backward is: [[ 0.6474455   1.26473622  0.20801781]\n",
      " [-1.58055568 -1.17139671 -2.63675753]\n",
      " [-0.08115182 -0.15172973  1.51355396]\n",
      " [-0.24373935  0.18095256  1.05577653]\n",
      " [-1.29046662 -1.46176374  1.36660152]\n",
      " [ 0.49211542  0.2739074   2.15750113]\n",
      " [ 0.52466266  0.51767449 -0.30038772]\n",
      " [ 1.57689859  1.40205788  2.18111823]\n",
      " [-0.8415869  -0.05082074  0.60534393]\n",
      " [-0.45143189  0.08380396  1.13489181]\n",
      " [-0.15971024 -0.37847382  0.57050881]\n",
      " [-1.98869502 -2.90491113 -0.38706513]\n",
      " [ 1.81243429  0.9124791   2.92566987]\n",
      " [ 0.06032388  0.12307115  0.30600583]\n",
      " [-0.45509639  0.11368819  1.36546385]\n",
      " [-1.40495237 -0.31849619  2.17872441]\n",
      " [-0.97813028 -1.02765696  1.46399332]\n",
      " [-0.0897674   0.03851995  0.47367823]\n",
      " [-0.50920135 -0.3295711   1.82892792]\n",
      " [-0.62133774 -0.19620488  1.27081969]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,5) (3,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib_test.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Compile and fit the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer, loss, initializer, X)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, X_val\u001b[39m=\u001b[39;49mX_val, y_val\u001b[39m=\u001b[39;49my_val, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vitor/OneDrive%20-%20PUCRS%20-%20BR/PUCRS/aprendizado_profundo/neuralNetworkImplementation/nnlib_v2/nnlib_test.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m model\u001b[39m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib\\models\\sequential.py:82\u001b[0m, in \u001b[0;36mSequentialModel.fit\u001b[1;34m(self, X, y, epochs, batch_size, X_val, y_val, verbose)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mon layer \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m the derivate on the backward is: \u001b[39m\u001b[39m{\u001b[39;00mdLda\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     81\u001b[0m         \u001b[39m# Update parameters\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mupdate(layer)\n\u001b[0;32m     84\u001b[0m \u001b[39m# Compute average loss for the epoch\u001b[39;00m\n\u001b[0;32m     85\u001b[0m avg_epoch_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(epoch_losses)\n",
      "File \u001b[1;32mc:\\Users\\vitor\\OneDrive - PUCRS - BR\\PUCRS\\aprendizado_profundo\\neuralNetworkImplementation\\nnlib_v2\\nnlib\\optimization_functions\\adam.py:34\u001b[0m, in \u001b[0;36mAdaptiveMomentEstimation.update\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Update moving averages of gradients and squared gradients\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm[\u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta_1 \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm[\u001b[39m'\u001b[39;49m\u001b[39mweights\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m (\u001b[39m1.\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta_1) \u001b[39m*\u001b[39;49m layer\u001b[39m.\u001b[39;49mderivative_weights\n\u001b[0;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv[\u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2 \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv[\u001b[39m'\u001b[39m\u001b[39mweights\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msquare(layer\u001b[39m.\u001b[39mderivative_weights)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mhas_bias:\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,5) (3,5) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Dummy classes for testing\n",
    "from nnlib.layers.dense import Dense\n",
    "\n",
    "from nnlib.activation_functions.linear import Linear\n",
    "from nnlib.activation_functions.sigmoid import Sigmoid\n",
    "from nnlib.activation_functions.relu import ReLu\n",
    "\n",
    "from nnlib.optimization_functions.adam import AdaptiveMomentEstimation\n",
    "\n",
    "from nnlib.models.sequential import SequentialModel\n",
    "\n",
    "from nnlib.loss_functions.mse import MeanSquaredError\n",
    "\n",
    "from nnlib.initialization_functions.he import He\n",
    "\n",
    "# Your SequentialModel class here...\n",
    "\n",
    "# Testing\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# Create dummy data\n",
    "X = np.random.randn(100, 3)\n",
    "y = np.random.randn(100, 1)\n",
    "X_val = np.random.randn(20, 3)\n",
    "y_val = np.random.randn(20, 1)\n",
    "\n",
    "optimizer = AdaptiveMomentEstimation()\n",
    "loss = MeanSquaredError()\n",
    "initializer = He()\n",
    "\n",
    "# Initialize model and components\n",
    "model = SequentialModel()\n",
    "model.add(Dense(n_units = 5, \n",
    "                input_dim = 3,\n",
    "                activation = Sigmoid(),\n",
    "                has_bias = True                 \n",
    "                ))\n",
    "model.add(Dense(n_units=5, \n",
    "                input_dim=1,\n",
    "                activation = Linear(),\n",
    "                has_bias = True\n",
    "                ))\n",
    "\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer, loss, initializer, X)\n",
    "model.fit(X, y, epochs=10, batch_size=20, X_val=X_val, y_val=y_val, verbose=True)\n",
    "\n",
    "model.best_params\n",
    "# Ensure to check the predictions, loss values, and best parameters during and after training.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
